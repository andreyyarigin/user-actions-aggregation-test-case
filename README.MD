# user-actions-aggregation-test-case

## Описание проекта
Приложение для пакетной обработки потока данных и вычисления агрегированных показателей на временном окне 7 дней. 

## Задача
Разработать приложение, которое будет обрабатывать логи CRUD действий пользователей в формате CSV и вычислять суммарное количество действий за последние 7 дней для каждого уникального пользователя.

## Входные данные
Логи представляют собой CSV файлы, каждый из которых содержит записи за один день с тремя колонками:
- `email`
- `action` (один из: CREATE, READ, UPDATE, DELETE)
- `dt` (дата и время)

Для генерации входных данных использован [скрипт](https://github.com/andreyyarigin/user-actions-aggregation-test-case/blob/main/scripts/generate_sample_data.py)

## Сценарий использования
Приложение вычисляет суммарное количество каждого действия для каждого пользователя за 7 предыдущих дней. Выходной CSV файл должен содержать одну строку для каждого уникального пользователя с почтой и количествами каждого из типов действий. 

Формат выходного файла:
- `email`
- `create_count` 
- `read_count`
- `update_count` 
- `delete_count`

Итоговый файл сохраняется в директорию `output` с именованием по правилу `YYYY-mm-dd.csv`. Например, если вычисление агрегата выполняется за 2024-09-16, то данные за период с 2024-09-09 по 2024-09-15 будут использованы, и результат будет записан в `output/2024-09-16.csv`.

## Реализованная функциональность
- Развернут Apache Airflow с одним DAG, который запускает вычисление недельного агрегата каждый день в 7:00.
- В качестве входных и выходных директорий используются директории на хост-машине, которые замонтированы в контейнер Spark